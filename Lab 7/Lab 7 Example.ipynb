{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> LAB 7: </span>\n",
    "# <span style='color:blue'> ATTENTION-MECHANISM TRANSFORMER ARCHITECTURE </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from typing import Iterable, List\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more on PyTorch Transformer support for language translation: https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the tokenization library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn what these are later. For now, your goal is to run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_token_transform = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "fr_token_transform = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error saying something along the lines of \"the computer can't find this\", it's because you haven't downloaded the tokenization libraries yet. To do that, you should open up command terminal and run the following code:\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bada bing bada boom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. Salut!\n",
      "Stop! Arrête-toi !\n",
      "I won! J'ai gagné !\n",
      "Get up. Lève-toi.\n",
      "Hop in. Montez.\n",
      "I paid. J’ai payé.\n",
      "No way! Il n'en est pas question !\n",
      "We won. Nous gagnâmes.\n",
      "Be fair. Soyez juste !\n",
      "Be nice. Soyez gentils !\n"
     ]
    }
   ],
   "source": [
    "# Load data as a list of tuples\n",
    "with open(\"en_to_fr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "text_pairs = []\n",
    "for l in lines:\n",
    "    appendage = l.split(\"\\t\")\n",
    "    text_pairs.append(appendage)\n",
    "\n",
    "# Print the first ten samples\n",
    "for i in range(10): \n",
    "    print(text_pairs[i][0], text_pairs[i][1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create source and target language tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Salut', '!']\n",
      "['Hi', '.']\n"
     ]
    }
   ],
   "source": [
    "# Download source and target language tokenizers\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'fr'\n",
    "\n",
    "token_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = en_token_transform\n",
    "token_transform[TGT_LANGUAGE] = fr_token_transform\n",
    "# hint: if you get an OSError, try running the following lines from the command line:\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download fr_core_news_sm\n",
    "\n",
    "en_0_tokenized = token_transform[SRC_LANGUAGE](text_pairs[0][1]) \n",
    "fr_0_tokenized = token_transform[TGT_LANGUAGE](text_pairs[0][0]) \n",
    "\n",
    "# Print the tokenized first line of each dataset\n",
    "print(en_0_tokenized) \n",
    "print(fr_0_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vocabulary for each language's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens: a \"unique\" filler-token to use when we can't tokenize a particular word, a \"padding\" token, a \"beginning-of-sentence\" token and an \"end-of-sentence\" token. \n",
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# And their special indeces in our vocabulary\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# A helper function that converts a list of strings into a list of lists-of-tokens\n",
    "def yield_tokens(data_iter, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        try:\n",
    "          yield token_transform[language](data_sample[language_index[language]])\n",
    "        except IndexError:\n",
    "          print(f\"token_transform.keys(): {token_transform.keys()}\")\n",
    "          print(f\"language: {language}\")\n",
    "          print(f\"data_sample: {data_sample}\")\n",
    "          print(f\"language_index: {language_index}\")\n",
    "          raise IndexError\n",
    "\n",
    "# Create a vocabulary object for each language using the build_vocab_from_iterator function\n",
    "vocab_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Invoke torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(text_pairs, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_tokens,\n",
    "                                                    special_first=True)\n",
    "\n",
    "\n",
    "# ``UNK_IDX`` is the index returned when the token is not found.\n",
    "# If we don't use this, we'll get a ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "# Let's see the first 20 words in each vocabulary\n",
    "print(vocab_transform[SRC_LANGUAGE].get_itos()[:20])\n",
    "print(vocab_transform[TGT_LANGUAGE].get_itos()[:20]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validate-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17563 total pairs\n",
      "14050 training pairs\n",
      "1756 validation pairs\n",
      "1757 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the text pairs\n",
    "shuffler = np.random.permutation(len(text_pairs))\n",
    "text_pairs = [text_pairs[i] for i in shuffler] \n",
    "\n",
    "# Train-validate-test split: 80-10-10\n",
    "n_train = int(0.8*len(text_pairs))\n",
    "train_pairs = text_pairs[:n_train] \n",
    "\n",
    "n_val = int(0.1*len(text_pairs))\n",
    "val_pairs = text_pairs[n_train:n_train+n_val]\n",
    "\n",
    "n_test = int(0.1*len(text_pairs))\n",
    "test_pairs = text_pairs[n_train+n_val:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "def generate_square_subsequent_mask(sz): \n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Mask function\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms): \n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# Adds BOS/EOS and creates a tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms convert the raw strings into tensors indices\n",
    "text_transform = {} \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# The \"collation\" function collates data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains the model for a single epoch\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    train_iter = train_pairs\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        # Since we're training, recall we need to mask our input\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        # What do you think the model does with the masks when it's in evaluation mode?\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    return loss_list\n",
    "\n",
    "# Evaluates the model\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "\n",
    "    val_iter = val_pairs\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PositionalEncoding module quantifies the relative position of words in a sentence\n",
    "# Notice that this is not actually an MLP or neural network, i.e. it has no learned parameters\n",
    "# it is just a function that you could represent analytically, if you wanted to\n",
    "class PositionalEncoding(nn.Module): # <-- \"Embedding\"\n",
    "    def __init__(self, emb_size, dropout, maxlen = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# The TokenEmbedding module converts a tensor of vocabulary-indices into a tensor of token-embeddings\n",
    "# Also not a neural network, but a lookup table\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Sequence-to-sequence transformer \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 embedding_size,\n",
    "                 num_heads,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 dim_feedforward = 512,\n",
    "                 dropout = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=embedding_size,\n",
    "                                       nhead=num_heads,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(embedding_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, embedding_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, embedding_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            embedding_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src,\n",
    "                trg,\n",
    "                src_mask,\n",
    "                tgt_mask,\n",
    "                src_padding_mask,\n",
    "                tgt_padding_mask,\n",
    "                memory_key_padding_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src)) # source sequence --> token embedding --> positional encoding\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg)) # target sequence --> token embedding --> positional encoding\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE]) \n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NUM_HEADS = 8 # Why 8? What do you expect to happen if we increase this parameter?\n",
    "FFN_HID_DIM = 512 \n",
    "BATCH_SIZE = 8\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model, loss function, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NUM_HEADS, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Epoch time = 234.264s\n",
      "Epoch: 2, Epoch time = 200.812s\n",
      "Epoch: 3, Epoch time = 184.146s\n",
      "Epoch: 4, Epoch time = 187.888s\n"
     ]
    }
   ],
   "source": [
    "# Fair warning: you might get an \"out of memory\" error\n",
    "# If that happens, try reducing the batch size\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    train_loss_list.extend(train_loss)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    val_loss_list.extend(val_loss)\n",
    "    print((f\"Epoch: {epoch}, Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "plt.scatter(range(len(train_loss_list)), train_loss_list, color = 'blue', linewidth = 3, alpha=0.1)\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks(ticks = [(i+1)*len(train_loss_list)//NUM_EPOCHS for i in range(NUM_EPOCHS)], labels=[f\"{i+1}\" for i in range(NUM_EPOCHS)])\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the last 10 target/translated sequences from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate output sequence using greedy algorithm\n",
    "# This basically saves us some compute time by taking a bunch of shortcuts (e.g. not computing the full softmax)\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "# Actual function to translate input sentence into target language\n",
    "# Translation function that actually uses the model to translate a sentence from source to target\n",
    "def translate(model, src_sentence):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "for i in range(10):\n",
    "    test_pair = test_pairs[-i]\n",
    "    test_str_de = test_pair[0]\n",
    "    test_str_en = test_pair[1]\n",
    "    print(f\"Target: {test_str_en}\")\n",
    "    print(\"Model output:\", translate(transformer, test_str_de))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
